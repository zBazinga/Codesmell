#    实验记录

**2020.6.22  更改输入**

  生成最新格式的数据集： id content context tag move-tag，问题在于处理切片 join函数的使用。

**2020.6.23 pytorch学习**

  学完了pytorch的autograd\网络模块

**2020.6.24 代码的网络结构学习/更改**

1. 搞清楚为什么输入x是【3129，128】为什么不是【128，3129】，这样的切分是会影响本来的顺序吗？

   不影响，他只是竖着切分而已。

2. Puermte（）

   修改维数的时候会顺带手修改数据吗，也就是把数据变回原来的位置。不会，不然数据就不够处理了。

3. 搞清楚TextCnn维度变化

   [3129,128] 进入嵌入矩阵变成[3129,128,300]，然后变成正常的[128,3129,300],然后长宽为[3x300][4x300\][4x300\]尺寸的卷积核各16个，每个核都出一个结果，所以有[128,16,句子长-卷积核+1,1\],[\],[\]总共48个feature map，然后把最后一维压缩变成[128,16,句子长-卷积核+1\],[\],[\].然后经过池化层，池化层就是一个feature map挑一个最大的,48个map就最后是48个数字，所以变成[128,16,1\],[12l8,16,1\],[128,16,1\]。再把最后的1维压缩变成[128,16\]，[128,16\]，[128,16\]。现在相当于把每句话变成了维度16的特征表示，最后cat一下变成[128,48\],再送入全联接层最后输出[128,2\]

4. 改动

   1. config.py里改变路径
   2. model.py里 _\_init__()self.fc  多乘了一个2 多增加了一个Modulist
   3. model.py里forward() 每个步骤都是相同的，只是在drop前进行了拼接，然后一样送进了全联接层
   4. Train.py里train()和eval()读取数据那块加了 batch.context  model()里也增加了输入

5. 上次实验记录

![image-20200624151046705](https://tva1.sinaimg.cn/large/007S8ZIlly1gg3ecba49vj312w090q9y.jpg)

6. 更改了 迭代次数为2000 更改了计算f1可能出现的除数为0的状况
7. 更改eval()函数返回 f1 recall precison， 在每次的验证时保存最好的f1对应的recall\precision,输入保存的最佳模型时除了输出f1，也输出recall precision
8. 弃用了 early_stopping参数，本来是会保存最好的，在这个参数步后仍然这个事最好的就停止，现在弃用，只有迭代次数完成的最好的才是最好的。

**2020.6.25 context概念问题**

1. 设定的迭代2000次，最后2000+步的时候已经过拟合，pre/rec/f1都接近1
2. 查询最好的模型保存在第600步左右，f1 59 recall 51 precision 0.717.后期f1验证集一直是0.51左右，训练集过拟合了。
3. ![image-20200625101045221](https://tva1.sinaimg.cn/large/007S8ZIlly1gg4baeft44j313g0autjm.jpg)
4. ![image-20200625100859697](https://tva1.sinaimg.cn/large/007S8ZIlly1gg4b8kky74j311u0a4gwv.jpg)
5. 看了老师发的几个博士答辩的ppt，刘广杰的2、3课题好像有点用。至于老师说的如何提出RQ，还得再考虑，目前没感觉，也记不住，写论文的时候看看其他人的就可以快速总结出来。现在还是先把模型跑起来。
6. 和光怡师姐聊了聊有关的东西，发了她的论文，先看，然后师姐给我她看过的论文。



**2020.7.6  数据集格式+网络问题**

1. 数据集改成  方法名/方法的返回值形参（context）/方法的调用/包含类名字/类context/目标类名字/目标类context

2. github备份至上一次 “2 input context"

3. <u>想法：可以把 方法名字作为i一个输入、方法上下文作为一个输入、方法调用一个输入，三个各自</u>（待定）

4. 现在是7个输入，各自卷积，之后是方法自己拼接，类自己拼接，最后一个方法（已经拼接加入context）的和两个类（已包含context)的拼接成一个向量【128，96*3】，送入全联接层

5. 改动包括：数据集文件的标签、已经排序函数里面的内容、model主要代码、config里的设置、train文件里的输入

6. 在方法那块是三个元素cat 所以维度[128,144]  别人都是两个拼在一起时[128,96],所以要考虑<u>是拼接还是相加的问题</u>

7. 开始用mac跑了一次，看起来提升很高

   ![image-20200706194951230](https://tva1.sinaimg.cn/large/007S8ZIlly1gghhudoxolj313k03qmxi.jpg)

**2020.7.7  挪到实验室电脑 跑实验** 

1. 出现问题：在eval()时候，类名字包含单词书太少，导致尺寸比卷积核5小，所以在convs6更改了卷积核。而且更改了相应的全联接层的尺寸。

2. 还是在eval()出问题，所以在convs4也改了卷积尺寸，去掉了5。 <u>为什么训练的时候不会碰到这种事情呢？</u>
3. ![image-20200707105657834](https://tva1.sinaimg.cn/large/007S8ZIlly1ggi826l0edj30ti03cdhh.jpg)

4. ![image-20200707112436251](https://tva1.sinaimg.cn/large/007S8ZIlly1ggi8uyc39ej311o06844s.jpg)

   在1030步时候已经拟合，迭代了15次就完成了拟合。

   ![image-20200707113713139](https://tva1.sinaimg.cn/large/007S8ZIlly1ggi982rp4oj310203ctbr.jpg)

**2020.7.8 没做**

**2020.7.9 attention**

1. 看了以前的attention理解，懂了基本概念，不知道怎么下手。<u>在考虑attention对这种短文本会不会有影响啊</u>[链接](https://blog.csdn.net/heyc861221/article/details/80128748?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-2.edu_weight&depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-2.edu_weight)

2. 博客介绍了两篇文本分类 一个是 textrnn+attention  一个 textrnn+cnn都需要看下，还有推荐知乎调参

3. 第一个用来处理名字的filter尺寸也大了，也改成了3,4

4. 在做图的时候想到应该也把class的上下文提取出来

5. 现在用目前的网络进行全部测试

    第一次 test1 train1       1030/15拟合

   ![image-20200707113713139](https://tva1.sinaimg.cn/large/007S8ZIlly1ggkntrqgjxj310203cq3a.jpg)

   第二次test2 train2         600左右就拟合了

   ![image-20200709180226426](https://tva1.sinaimg.cn/large/007S8ZIlly1ggkvlhmolij311a05cjvj.jpg)

   第三次   490左右拟合

   ![image-20200709182824496](https://tva1.sinaimg.cn/large/007S8ZIlly1ggkwci4asmj312k06e0yz.jpg)

   第四次 800左右拟合

   ![image-20200709191538830](https://tva1.sinaimg.cn/large/007S8ZIlly1ggkxpo34b0j311c068jyb.jpg)

   第五次 1000左右拟合

   ![image-20200709210031644](https://tva1.sinaimg.cn/large/007S8ZIlly1ggl0qsasq7j314y076n56.jpg)

   第六次 400左右拟合

   ![image-20200709214038769](https://tva1.sinaimg.cn/large/007S8ZIlly1ggl1wjb5yuj310u068q9t.jpg)

   第七次 1000左右拟合

   ![image-20200709225251997](https://tva1.sinaimg.cn/large/007S8ZIlly1ggl3zoe52ij311i070qas.jpg)

   第八次 600左右拟合

   ![image-20200709233504050](https://tva1.sinaimg.cn/large/007S8ZIlly1ggl57lds83j312m07ydpd.jpg)

   第九次 1000左右拟合

   ![image-20200710082525416](https://tva1.sinaimg.cn/large/007S8ZIlly1gglkjf93sjj3114066gs3.jpg)

   第十次 700左右拟合

   ![image-20200710090603874](https://tva1.sinaimg.cn/large/007S8ZIlly1ggllppdiapj311g06wdnn.jpg)

**2020.7.10  开会**

1. 老师说你的东西得有对这个领域的启示，我这边考虑我把简单的fe从调用变成了在某种环境下的调用，这种环境的匹配性（上下文）其实有搞头，或者说不单单去考虑调用之间造成的fe，还有上下文环境之间的错乱导致的问题。
2. 老师说最初的“什么情况下容易出现坏味”，不就是前面说的 我观察的上下文调用，也就是说不光看调用的混乱，还可以看在上下文调用混乱的时候会增加坏味出现的概率。
3. ![image-20200710115652426](https://tva1.sinaimg.cn/large/007S8ZIlly1gglqnfipf0j30w80mo47i.jpg)

![image-20200710115719329](https://tva1.sinaimg.cn/large/007S8ZIlly1gglqnw9fxij30zm0mowlu.jpg)

![image-20200710115750408](/Users/dashabi/Library/Application Support/typora-user-images/image-20200710115750408.png)![image-20200710115750450](https://tva1.sinaimg.cn/large/007S8ZIlly1gglqogve4sj30vk0k2wkx.jpg)

4. 类的上下文是什么种类的会导致坏味概率的上升，别人做的时候的看到什么样的会觉得有坏味。
5. 之后就是 一方面把模型高级点  二是把baseline方法实践下 三是从画图开始，把问题故事写好。
6. 更改了信息，提取出了类的上下文信息做成表格。
7. 更改了生成 dataset的路径以及读取dataset路径和生成的最终数据的路径.最终包含类的context的数据都存在文件夹final_dataset_context.
8. 最终两个表内数据条数对不上，有两个文件是原来计算有错，有一个是差了8条，原因是以前为了凑10-fold自己删除了几条。所以新生成的一个文件 actor-platform也删除8条，凑成9380，可以被10整除。

**2020.7.13 attention**

1. 本来打算git同步两台电脑，弄了一早上总是出错，烦死了。算了，以后只有mac push 同步修改。